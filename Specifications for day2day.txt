Specifications for issue_observatory_search

You are a skilled developer and have to help me create a library for an application called issue_observatory_search. The application should have a front end, but the code should also be usable purely as a backend API. The main backend language is Python. It should be based on the existing codebase found in the current directory, however much additional work is needed. It should serve mainly as inspiration. You are allowed to create new files, edit old ones, re-arrange the folder structure and help determine the best way to organize the code. Also feel free to make suggestions before implementing. Ideally the code is commited to git in a number of steps instead all of it at once. The code should meet the following requirement:

- Allow users to search for websites based on a list of search terms and then scrape the textual content from those websites. The search terms are sequentially sent to a search engine which returns links to websites. These websites are then scraped using selenium in python. The code should be structured in a way that allow develops to add additional search engine integration. Right now two search integrations should be available: 1) Google search via the customsearch API and Google search via SERP API. The data from the scrapes should be stored and attached to the specific user that made the scrape. If the same website is found with multiple searches it should not be scraped twice. The user can change the following parameters for this main function:

	1. Which search integration to use
	2. How many of the top results delivered by the search engine for each of the search terms. Between 1 and 200.
	2. Which domains are allowed, for example [.dk, .de, .com]. Default is to allow all domains, expressed as an empty list. 
	3. How deep the scrape should be, either level 1 which scrapes only the URL returned by the search engine, or level 	2 which also scrapes additional URLs from the same domain found on the URL returned by the search engine. And level 3 which attempts to follow as many unique URLs from the same domain as possible and scrape them all for text until no new ones appear.


- Allow users to export a network file (.gexf) which is based on the data of one or more scrapes. The network should represent relations between websites and the content. The code should make it easy for developers to add multiple network building protocols. For now only one primary network building protocol should be available, a bipartite network where nodes represent websites and nouns and edges are drawn between websites that contain the nouns found in the websites' texts. The occurences of nouns should be TF-IDF weighted and for each website only the top nouns should be included. The user can change the following parameters for this main function:

	1. Which network building protocol to use. Only one (bipartite where nodes are websites and nouns) is available at the moment.
	2. The languages to be included. Multiple languages possible. Default is Danish.
	3. The top n nouns to include from each website in the network. Can be expressed either as a total number or proportion (n<1). Default is 10.

The application should have a front end that allow users to log in and scrape data and export networks files. Scrapes made by one user should be accessible only to that user. Search terms can be uploaded as a csv file and stored on the server. Network graphs can be build and exported based on any combination of succesfully scraped search terms. At the moment users cannot be created via front end, but must be added manually by backend admins. The database and frontend-backend integration framework should be build with Flask, SQLite and Flask-login using Vanilla JS + HTMX as the main frontend framework.

